{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from time import sleep\n",
    "import itertools\n",
    "from datetime import timedelta\n",
    "import diffdist.functional as distops\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from time import sleep\n",
    "import itertools\n",
    "from datetime import timedelta\n",
    "import diffdist.functional as distops\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.autograd import Variable\n",
    "import torch.distributed.rpc as rpc\n",
    "from torch.distributed.optim import ZeroRedundancyOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConvModel(nn.Module):\n",
    "    def __init__(self, ps, input_num_filters, output_num_filters, filter_size):\n",
    "        super(MyConvModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_num_filters, output_num_filters, kernel_size=(filter_size, filter_size))\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        return x\n",
    "\n",
    "class MyOutputModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, ouput_size):\n",
    "        super(MyOutputModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, ouput_size)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parameter_rrefs(module):\n",
    "    param_rrefs = []\n",
    "    for param in module.parameters():\n",
    "        param_rrefs.append(RRef(param))\n",
    "    return param_rrefs\n",
    "\n",
    "class MyPipelineModel(nn.Module):\n",
    "    def __init__(self, num_worker, split_size=32):\n",
    "        super(MyPipelineModel, self).__init__()\n",
    "        #self.models = models\n",
    "        self.num_worker = num_worker\n",
    "        self.split_size = split_size\n",
    "        self.optimizer = ZeroRedundancyOptimizer(optim.SGD, self.parameter_rrefs(), lr=0.001,)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.epochs = 5\n",
    "        #self.batch_size = 100\n",
    "        self.running_loss = 0\n",
    "        \n",
    "        self.model1 = rpc.remote(trainee0, MyConvModel, args=(1, 16, 5))\n",
    "        self.model2 = rpc.remote(trainee1, MyConvModel, args=(16, 32, 5))\n",
    "        self.model3 = MyOutputModel(512, 128, 10)\n",
    "        \n",
    "    def run(self):\n",
    "        processes = []\n",
    "        \n",
    "        for rank in range(self.num_worker):\n",
    "            p = mp.Process(target=self.init_process, args=(rank, self.num_worker))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "            \n",
    "    def parameter_rrefs(self):\n",
    "        remote_params = []\n",
    "        remote_params.extend(_remote_method(_parameter_rrefs, self.model1))\n",
    "        remote_params.extend(_remote_method(_parameter_rrefs, self.model2))\n",
    "        remote_params.extend(_parameter_rrefs(self.model3))\n",
    "        \n",
    "        return remote_params\n",
    "        \n",
    "    def init_process(rank, world_size):\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = '29500'\n",
    "        if rank == world_size - 1:\n",
    "            rpc.init_rpc(\"trainer\", rank=rank, world_size=world_size)\n",
    "            self.train()\n",
    "        else:\n",
    "            rpc.init_rpc(\"trainee\" + str(rank), rank=rank, world_size=world_size)\n",
    "            # parameter server do nothing\n",
    "            pass\n",
    "\n",
    "        # block until all rpcs finish\n",
    "        rpc.shutdown()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output1 = _remote_method(MyConvModel.forward, self.model1, input)\n",
    "        output2 = _remote_method(MyConvModel.forward, self.model2, output1)\n",
    "        return self.model3(output2)\n",
    "        \n",
    "    def train(self, rank):\n",
    "        x = trainloader\n",
    "        time0 = time()\n",
    "        for e in range(self.epochs):\n",
    "            self.running_loss = 0\n",
    "            \n",
    "            for images, labels in x:\n",
    "                with dist_autograd.context() as context_id:\n",
    "                    output = self(imgages)\n",
    "                    loss = self.criterion(output, labels)\n",
    "                    dist_autograd.backward(context_id, [loss])\n",
    "                    slef.optimizer.step(context_id)\n",
    "                    self.running_loss += loss.item()\n",
    "            \n",
    "            if rank == self.num_worker-1:\n",
    "                print(\"Epoch {} - Training loss: {}\".format(e, self.running_loss/len(trainloader)))\n",
    "                print(\"\\nTraining Time (in seconds) =\",(time()-time0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyPipelineModel(3)\n",
    "model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
